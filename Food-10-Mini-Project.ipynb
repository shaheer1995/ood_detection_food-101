{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gi5R0HWIrlOy","outputId":"81330c9d-0c9f-4351-c757-91380200d388"},"outputs":[{"name":"stdout","output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com\u0026redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob\u0026scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly\u0026response_type=code\n","\n","Enter your authorization code:\n","4/1AY0e-g5V04ijG5xunQSor2KVgufELmNt2epwAvuLw_zMV8NJBU2qsoriOBY\n"]}],"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnJH-dtjsCiP"},"outputs":[],"source":["!cp /content/drive/MyDrive/Research/datasets/food-10.zip /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFcv0sWzs2nA"},"outputs":[],"source":["!unzip food-10.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DHCY1urtfJW"},"outputs":[],"source":["# Check the contents of images folder\r\n","!ls food-10/images\r\n","# Check the contents of images folder\r\n","!ls food-10/meta\r\n","# Check the contents of the downloaded file\r\n","!ls food-10/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1wJA6VxtMbCoOpmECBf4IlBvVVocDLiBJ"},"id":"KbARpRqUtjd6","outputId":"7a79dca9-4d22-4d8b-9249-f610d9442196"},"outputs":[],"source":["import matplotlib.pyplot as plt\r\n","import matplotlib.image as img\r\n","%matplotlib inline\r\n","import numpy as np\r\n","from collections import defaultdict\r\n","import collections\r\n","import os\r\n","\r\n","# Visualize the data, showing one random image per class from 101 classes\r\n","rows = 3\r\n","cols = 8\r\n","fig, ax = plt.subplots(rows, cols, figsize=(50,50))\r\n","fig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24) \r\n","data_dir = \"food-10/images/\"\r\n","foods_sorted = sorted(os.listdir(data_dir))\r\n","food_id = 0\r\n","for i in range(rows):\r\n","  for j in range(cols):\r\n","    try:\r\n","      food_selected = foods_sorted[food_id] \r\n","      food_id += 1\r\n","    except:\r\n","      break\r\n","    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\r\n","    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\r\n","    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\r\n","    ax[i][j].imshow(img)\r\n","    ax[i][j].set_title(food_selected, pad = 10)\r\n","    \r\n","plt.setp(ax, xticks=[],yticks=[])\r\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Xg8YFrdAt2t9"},"outputs":[],"source":["# Helper method to split dataset into train and test folders\r\n","from shutil import copy\r\n","def prepare_data(filepath, src, dest):\r\n","  classes_images = defaultdict(list)\r\n","  with open(filepath, 'r') as txt:\r\n","      paths = [read.strip() for read in txt.readlines()]\r\n","      for p in paths:\r\n","        food = p.split('/')\r\n","        classes_images[food[0]].append(food[1] + '.jpg')\r\n","\r\n","  for food in classes_images.keys():\r\n","    print(\"\\nCopying images into \",food)\r\n","    if not os.path.exists(os.path.join(dest,food)):\r\n","      os.makedirs(os.path.join(dest,food))\r\n","    for i in classes_images[food]:\r\n","      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\r\n","  print(\"Copying Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"whkpVL5PTngk"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating train data...\n","\n","Copying images into  apple_pie\n","\n","Copying images into  chocolate_cake\n","\n","Copying images into  fried_rice\n","\n","Copying images into  ice_cream\n","\n","Copying images into  samosa\n","\n","Copying images into  chicken_wings\n","\n","Copying images into  donuts\n","\n","Copying images into  hotdog\n","\n","Copying images into  pizza\n","\n","Copying images into  sushi\n","Copying Done!\n","Creating test data...\n"]},{"ename":"IndexError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-7-41732ac3c851\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 7\u001b[0;31m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'food-10/meta/test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'food-10/images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'food-10/test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-6-adb90a4a99d8\u003e\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(filepath, src, dest)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m         \u001b[0mclasses_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfood\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\r\n","print(\"Creating train data...\")\r\n","prepare_data('food-10/meta/train.txt', 'food-10/images', 'food-10/train')\r\n","\r\n","# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\r\n","print(\"Creating test data...\")\r\n","prepare_data('food-10/meta/test.txt', 'food-10/images', 'food-10/test')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3lWi64sHTwMU"},"outputs":[],"source":["# Check how many files are in the train folder\r\n","print(\"Total number of samples in train folder\")\r\n","!find food-10/train -type d -or -type f -printf '.' | wc -c\r\n","\r\n","# Check how many files are in the test folder\r\n","print(\"Total number of samples in test folder\")\r\n","!find food-10/test -type d -or -type f -printf '.' | wc -c"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qydz57IXUbwT"},"outputs":[],"source":["# Import necessary PyTorch libraries\r\n","import torch\r\n","from torch import nn\r\n","from torch import optim\r\n","import torch.nn.functional as F\r\n","from torchvision import datasets, transforms, models\r\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RqDSlvi9Utye"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is not available.  Training on CPU ...\n"]}],"source":["# Check whether GPU is available and if it is then use GPU\r\n","train_on_gpu = torch.cuda.is_available()\r\n","\r\n","if not train_on_gpu:\r\n","    print('CUDA is not available.  Training on CPU ...')\r\n","else:\r\n","    print('CUDA is available!  Training on GPU ...')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0ujGXO7PXlLL"},"outputs":[],"source":["# Data augmentation\r\n","train_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\r\n","                                       transforms.RandomHorizontalFlip(),\r\n","                                       transforms.RandomVerticalFlip(),\r\n","                                       transforms.RandomRotation(45),\r\n","                                       transforms.RandomAffine(45),\r\n","                                       transforms.ColorJitter(),\r\n","                                       transforms.ToTensor(),\r\n","                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n","                                                            std=[0.229, 0.224, 0.225])])\r\n","\r\n","# Use 10-crop for Test Time Augmentation\r\n","test_transforms = transforms.Compose([transforms.Resize(256),\r\n","                                      transforms.TenCrop(224),\r\n","                                      transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\r\n","                                      transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])(crop) for crop in crops]))])\r\n","\r\n","# Load the datasets with ImageFolder\r\n","train_data = datasets.ImageFolder(\"./food-10/train\", transform=train_transforms)\r\n","test_data = datasets.ImageFolder(\"./food-10/test\", transform=test_transforms)\r\n","\r\n","# Using the image datasets and the tranforms, define the dataloaders\r\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size = 64, shuffle = True)\r\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size= 64, shuffle = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Nfj1uTi_XrJ4"},"outputs":[],"source":["# Load the ResNet-50 model pretraned on ImageNet \r\n","model = models.resnet50(pretrained=True)\r\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0tk2BWxgXzbI"},"outputs":[],"source":["# Freeze the initial layers\r\n","for param in model.parameters():\r\n","  param.requies_grad = False\r\n","  \r\n","# Replace the final fully connected layer with new fully connected layer (randomly initialized weights)\r\n","# that outputs 101 units (based on our dataset)\r\n","classifier = nn.Linear(2048, 10)\r\n","model.fc = classifier\r\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bnLKLiRVX4Va"},"outputs":[],"source":["# specify loss function (categorical cross-entropy)\r\n","criterion = nn.CrossEntropyLoss()\r\n","\r\n","# specify optimizer (stochastic gradient descent with momentum)\r\n","optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\r\n","\r\n","# secify learning rate scheduler (if there is no further decrease in loss for next 5 epochs \r\n","# then lower the learning rate by 0.1)\r\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TEV4QLIyX8m-"},"outputs":[],"source":["# Move the model to GPU if CUDA is available\r\n","if train_on_gpu:\r\n","    model.cuda()\r\n","\r\n","# number of epochs to train the model\r\n","n_epochs = 10\r\n","\r\n","valid_loss_min = np.Inf # track change in validation loss\r\n","\r\n","for epoch in range(1, n_epochs+1):\r\n","    # keep track of training and validation loss\r\n","    train_loss = 0.0\r\n","    train_accuracy = 0.0\r\n","    valid_loss = 0.0\r\n","    val_accuracy = 0.0\r\n","    \r\n","    ###################\r\n","    # train the model #\r\n","    ###################\r\n","    model.train()\r\n","    for data, target in train_loader:\r\n","        # move tensors to GPU if CUDA is available\r\n","        if train_on_gpu:\r\n","            data, target = data.cuda(), target.cuda()\r\n","        # clear the gradients of all optimized variables\r\n","        optimizer.zero_grad()\r\n","        # forward pass: compute predicted outputs by passing inputs to the model\r\n","        output = model(data)\r\n","        # calculate the batch loss\r\n","        loss = criterion(output, target)\r\n","        # backward pass: compute gradient of the loss with respect to model parameters\r\n","        loss.backward()\r\n","        # perform a single optimization step (parameter update)\r\n","        optimizer.step()\r\n","        # update training loss\r\n","        train_loss += loss.item()*data.size(0)\r\n","        # Calculate training accuracy\r\n","        top_p, top_class = output.topk(1, dim=1)\r\n","        equals = top_class == target.view(*top_class.shape)\r\n","        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()*data.size(0)\r\n","        \r\n","    ######################    \r\n","    # validate the model #\r\n","    ######################\r\n","    model.eval()\r\n","    with torch.no_grad():\r\n","        for data, target in test_loader:\r\n","            # move tensors to GPU if CUDA is available\r\n","            if train_on_gpu:\r\n","                data, target = data.cuda(), target.cuda()\r\n","            \r\n","            ## For 10-crop Testing\r\n","            input_var = torch.autograd.Variable(data, volatile=True)\r\n","            target_var = torch.autograd.Variable(target, volatile=True)\r\n","            bs, ncrops, c, h, w = input_var.size()\r\n","            # forward pass: compute predicted outputs by passing inputs to the model\r\n","            temp_output = model(input_var.view(-1, c, h, w))\r\n","            output = temp_output.view(bs, ncrops, -1).mean(1)\r\n","            # calculate the batch loss\r\n","            loss = criterion(output, target_var)\r\n","            # update average validation loss \r\n","            valid_loss += loss.item()*data.size(0)\r\n","            # Calculate validation accuracy\r\n","            top_p, top_class = output.topk(1, dim=1)\r\n","            equals = top_class == target.view(*top_class.shape)\r\n","            val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()*data.size(0)\r\n","    \r\n","    # calculate average losses and accuracies\r\n","    train_loss = train_loss/len(train_loader.dataset)\r\n","    train_accuracy = train_accuracy/len(train_loader.dataset)\r\n","    valid_loss = valid_loss/len(test_loader.dataset)\r\n","    val_accuracy = val_accuracy/len(test_loader.dataset)\r\n","    # Learning rate scheduler step\r\n","    scheduler.step(valid_loss) \r\n","    \r\n","    # print training/validation statistics \r\n","    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(\r\n","        epoch, train_loss, train_accuracy, valid_loss, val_accuracy))\r\n","    \r\n","    # save model if validation loss has decreased\r\n","    if valid_loss \u003c= valid_loss_min:\r\n","        print('Validation loss decreased ({:.6f} --\u003e {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\r\n","        \r\n","        checkpoint = {\"model\": model,\r\n","                      \"criterion\": criterion,\r\n","                      \"epochs\": epoch,\r\n","                      \"optimizer_state\": optimizer.state_dict(),\r\n","                      \"model_state\": model.state_dict(),\r\n","                      \"valid_loss_min\": valid_loss}\r\n","    \r\n","        # save model in colab\r\n","        torch.save(checkpoint, 'res50_model_checkpoint.pth')\r\n","        valid_loss_min = valid_loss"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM86N14kchItdW/E56s+GmF","name":"Food-10-Mini-Project.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}